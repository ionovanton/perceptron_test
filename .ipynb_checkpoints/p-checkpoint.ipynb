{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89956ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f228e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.array(pd.read_csv('./emnist-letters-test.csv'))\n",
    "data_train = np.array(pd.read_csv('./emnist-letters-train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0ee56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorized_answer(i: int):\n",
    "    v = np.zeros( (26, 1) )\n",
    "    v[i - 1] = 1\n",
    "    return v\n",
    "\n",
    "def tidy_data(data):\n",
    "    data_tidy = [(np.reshape(x[1:], (784, 1)), vectorized_answer(x[0])) for x in data]\n",
    "    return data_tidy\n",
    "\n",
    "data_test_list = tidy_data(data_test)\n",
    "data_train_list = tidy_data(data_train)\n",
    "a, b = data_test_list[0]\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f370724c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff1d4761b40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAAShUlEQVR4nO3de4xc1X0H8O93Zx+218/1C7/A2LXBDhCTLCYShBpDeLioQFsFXCkiLa2jKlSkQQqIVgqKKpVGTShKWypTXBxECLRgYUW0xThQQkVd1sj4ycM4Nt7F9hqMvfYaex/z6x97STaw53eXuTNzZznfj7Ta3fubO3P27n73zsy55xyaGUTks68u7waISHUo7CKRUNhFIqGwi0RCYReJRH01H6yRTTYKzdV8SJGonEI3euw0h6plCjvJawDcD6AA4F/M7F7v9qPQjIt5RZaHFBHHJtsYrJX8NJ5kAcA/ArgWwGIAK0kuLvX+RKSysrxmXwpgt5ntMbMeAD8FcH15miUi5ZYl7LMA7B/0fXuy7TeQXEWyjWRbL05neDgRyaLi78ab2WozazWz1gY0VfrhRCQgS9g7AMwZ9P3sZJuI1KAsYX8FwAKSZ5NsBHAzgPXlaZaIlFvJXW9m1kfyNgD/hYGutzVmtqNsLRORssrUz25mzwB4pkxtEZEK0uWyIpFQ2EUiobCLREJhF4mEwi4SCYVdJBJVHc8uMhgbGjPtb/39ZWrJEIoVvO+c6MwuEgmFXSQSCrtIJBR2kUgo7CKRUNhFIqGut9jVFdwyC34ddUPOWvzr/RvD3Ws8c6Z/3ynY1V36zikLmhaPdfm7n/KnWEvtFsyha09ndpFIKOwikVDYRSKhsItEQmEXiYTCLhIJhV0kEupnHwFY7/+aCjPOCNaKk8a6+x6+aJJbP7rI74+2lL+g4phwf/LKizb5O6d4qXO+W++38DUA/UX/PHeo3b8GoNDlX38w86WiWx+36Z1gre/AQXffUunMLhIJhV0kEgq7SCQUdpFIKOwikVDYRSKhsItEQv3s5UB/THf9LL/Pttgyzq0fvtjvC6+/8XCwduVMfxXt68ZvceuLGnvcegH+z+4ZzWxTSfdN25xpf0/vBf548z19/v43Hf+2Wx/71oRwsUL97JnCTnIvgOMA+gH0mVlrORolIuVXjjP75Wb2XhnuR0QqSK/ZRSKRNewG4FmSm0muGuoGJFeRbCPZ1gt/3i4RqZysT+MvNbMOktMAbCD5upm9OPgGZrYawGoAGM8Wf1SFiFRMpjO7mXUknzsBrAOwtByNEpHyKznsJJtJjvvoawBXAdheroaJSHlleRo/HcA6DvQx1wP4iZn9Z1laNcLUjRnj1t9bfqZbP7rAv/8pFx1y699f+O/B2rz6k+6+LYUmt14Pv56neqTMae8own9Fecr86wvWHbvIrY/f498/j51w65VQctjNbA+Az5exLSJSQep6E4mEwi4SCYVdJBIKu0gkFHaRSGiI6zB53WsHv+53Stz+5+GuMQBYPmaPW5+e2j3mdUH53YKVVmDp55N+86djTvNB8cNg7Z8/+KK770/e9Adwzrq/wa23bH7Nrfd9GG5bpejMLhIJhV0kEgq7SCQUdpFIKOwikVDYRSKhsItEQv3sibRlkeumTg7WTl/e5e57dUo/+pTCaP+xM0zX3Ad/SuR+84diNjHbn0jWvnJP2jDV507ODtb+9bll7r6Tt/rHvOE1f4ru/pP+0OI86MwuEgmFXSQSCrtIJBR2kUgo7CKRUNhFIqGwi0Qinn72lGWVed5Ct96+bGKw9uAX/sHdd1oh25jy0+avD7yue0awtmb/Je6+e/ZNc+t/c+mTbn1ug7+m5zt9LcHa8X7/+oK5jeGlqAFgb89Ut/6jf/q9YO2ctf4SB8WUfvL+vpQ1m2uQzuwikVDYRSKhsItEQmEXiYTCLhIJhV0kEgq7SCSi6WevPzM8thkAfvlX/v+975wXnvt9aZM/rhop49HTxpz/yb6r3fqbD58brE16/ZS77+Ld7W79e3+80q03+kP5MXnn6WCtvrvX3Xf3zf71CZwavm8AWPgfB4K1/q6Uhn8GpZ7ZSa4h2Uly+6BtLSQ3kHwr+Typss0UkayG8zT+YQDXfGzbXQA2mtkCABuT70WkhqWG3cxeBHDkY5uvB7A2+XotgBvK2ywRKbdSX7NPN7OPXhAdBDA9dEOSqwCsAoBROa87JhKzzO/Gm5kB4Zn/zGy1mbWaWWsD/AUKRaRySg37IZIzACD53Fm+JolIJZQa9vUAbkm+vgXA0+VpjohUSuprdpKPAVgGYArJdgDfBXAvgCdI3gpgH4CvVrKR5dAzJzzvOwD84cL/detXOnO/12V8L6LX/H72l9+Y79bP2XIifN/jG/0Hr/P/38942e/LbjoUfmwAQMfBcK3grSsP1Hef49Z7W1Lm00+ZwyA2qWE3s9BVFVeUuS0iUkG6XFYkEgq7SCQUdpFIKOwikVDYRSLxmRniyga/i+mdq/1pi2+d2ObWs0wHvbnH71p79vgS/7FfaHDrdSeOBWvd5zS7+za9M8qt17+wxa33F/2fzVP4nN+11jPVn6557Ha/7fbuoU/dps8yndlFIqGwi0RCYReJhMIuEgmFXSQSCrtIJBR2kUiMqH521oebW5gZnBkLADDlIr/PtaVQ+iw6aUsq3/TCn7n12ev9X8Pk53e6dTaH+9In/8KfStoa/Mfm58PTVA9LITzMdP/y8e6ut1/yjFt/6qwL3frpV8Ntb3zfPy51Pf7v1H65362nLfmcB53ZRSKhsItEQmEXiYTCLhIJhV0kEgq7SCQUdpFIjKx+9qZwX3jaVNG/M/Nlt14Pf1pjTxFFt153zB+PPqaj238A5/oCADh17ozwY/f4bfvgHH9M+NFz/eWoLe104dTnnucvF71i7A63PvPsD9z6nTeFZzgvHBvn7lvf7f9gZ/9byjTVO97w65a2zHf56cwuEgmFXSQSCrtIJBR2kUgo7CKRUNhFIqGwi0RiZPWzz50drO2/yp8X/o8mbk6592zLLrv8rm6w379B8awz3Hr3HeF546+bvd3d98qUvuxFjT1uvYDSl0VuYMqSzfB/p/Pr/X72Fdf+KFjrh9/PfSxlPvyrF/tzFMz7i5luve9dZynrDHPxe1LP7CTXkOwkuX3QtntIdpDcknysqEjrRKRshvM0/mEA1wyx/T4zW5J8+FOKiEjuUsNuZi8COFKFtohIBWV5g+42kluTp/mTQjciuYpkG8m2XpzO8HAikkWpYX8AwHwASwAcAPCD0A3NbLWZtZpZawNKn9RRRLIpKexmdsjM+s2sCOBBAEvL2ywRKbeSwk5y8JjKGwH4/TsikrvUfnaSjwFYBmAKyXYA3wWwjOQSAAZgL4BvVK6Jv1ZsDDe3d6zfbzquLtslBQU6/xczDk22gv8/993L/LHXjyy6L1j7rQa/caPpr2uPHF96ucccQL/51yek/2xhYwr+cVsxz78+YdeEBW6dB8PXJ6T8WCVLTYCZrRxi80MVaIuIVJAulxWJhMIuEgmFXSQSCrtIJBR2kUiMqCGulZTWzVNJTOm6G3PIv8EvTi4M1rpG73P3PbfBn8Z6TMow1PRhqqVP0Z3WtZbFh+YP3T1S9JdsfmbP59z6vGP+cBIraippEakQhV0kEgq7SCQUdpFIKOwikVDYRSKhsItEQv3sVVAc508N3DPBH4o5+Wevu/XHTw41H+iAtRP9/+fH57pl9I3z+4MvuXinW//2GRuCtfMb/aWss3qyOzhbGu58PrycMwA0dvrRmPe4P411X8e7bl1LNotIxSjsIpFQ2EUiobCLREJhF4mEwi4SCYVdJBLqZ0+kjZ32xrunTVn8t7/9hFt/YO4yt975xLlufeor4SWbJxw94e97+H23zoI/Hv1/vrfYrS+/JnyNwPmNzrLFw5A2Jv3O/w73pS+6+21332KXf9yKvf5j1yKd2UUiobCLREJhF4mEwi4SCYVdJBIKu0gkFHaRSHxm+tmZMsV4f8Z1lbPMYb58tD+2edb8dW79O7//B259z5nTg7Vxeye4+059ZYxbx7uH/XoN48nwNQLWfdLd10ZgP3qa1DM7yTkknye5k+QOkrcn21tIbiD5VvI5PFOAiORuOE/j+wDcYWaLAXwJwDdJLgZwF4CNZrYAwMbkexGpUalhN7MDZvZq8vVxALsAzAJwPYC1yc3WArihQm0UkTL4VK/ZSc4FcCGATQCmm9mBpHQQwJAvHEmuArAKAEYh5fWhiFTMsN+NJzkWwJMAvmVmXYNrZmbA0O+AmdlqM2s1s9YGNGVqrIiUblhhJ9mAgaA/amZPJZsPkZyR1GcA6KxME0WkHFKfxpMkgIcA7DKzHw4qrQdwC4B7k89PV6SFgxQOHw3WJm8d7+7791/5oltfNen/3Pq4uvChShviOqlutFv/UsoTnp+f/7hbP7L4dLC2r89/7J91LXHrj26+2K1/ebE/lfSSpv1O1Z9Kuqt4yq3v6/OH39ZNDe/PCf7fC06HjymAXKaCzmo4r9kvAfA1ANtIbkm23Y2BkD9B8lYA+wD4E3GLSK5Sw25mLwFgoHxFeZsjIpWiy2VFIqGwi0RCYReJhMIuEgmFXSQSI2qIa/FIeJncyZvHuvs+8txlbr33Cr/PdpEzTHX56H3uvl4f/XCk9eNPK4QvQ24p+MtFn9Xyslu/4MvvuPWFDf61VGc3hIcGn7Y+d98NH85w608d9q+dsEOjwsXiyOsnz0pndpFIKOwikVDYRSKhsItEQmEXiYTCLhIJhV0kErQqjssdzxa7mPkMlKtrbvZvMH+OW+6dHO7Lbr/cH5DeO94/xsVmvy/8ry97yq1f19werBWCAxaHJ20K7l09/jUAb/dOC9Z+9Pbl7r7FJ6a69ZZtXW69rj08DXZ/Z8oU2SNwvDoAbLKN6LIjQ/7SdWYXiYTCLhIJhV0kEgq7SCQUdpFIKOwikVDYRSIRTT97JbHB72tGnd/XXTfaGXcN4P3fXezXLwj/Di3jv/O0pbAn7vJ/tol7wvOvN+32x8L3Hzjo1q3PHw8fI/Wzi4jCLhILhV0kEgq7SCQUdpFIKOwikVDYRSIxnPXZ5wD4MYDpAAzAajO7n+Q9AP4UwEcDg+82s2cq1dBaZr09mfbv7/H3n/Jzf+72SdsmZnr8LArvHXPrxaPhel/3Sf/Oi/44f/l0hrN6QR+AO8zsVZLjAGwmuSGp3Wdmf1e55olIuQxnffYDAA4kXx8nuQvArEo3TETK61O9Zic5F8CFADYlm24juZXkGpKTAvusItlGsq0X4UsnRaSyhh12kmMBPAngW2bWBeABAPMBLMHAmf8HQ+1nZqvNrNXMWhvgz9UmIpUzrLCTbMBA0B81s6cAwMwOmVm/mRUBPAhgaeWaKSJZpYadJAE8BGCXmf1w0PbBS2zeCGB7+ZsnIuUynHfjLwHwNQDbSG5Jtt0NYCXJJRjojtsL4BsVaF8cUoYZ97V3+Pun1StIg0xHjuG8G/8SMOTk41H2qYuMVLqCTiQSCrtIJBR2kUgo7CKRUNhFIqGwi0RCYReJhMIuEgmFXSQSCrtIJBR2kUgo7CKRUNhFIqGwi0Siqks2kzwMYN+gTVMAvFe1Bnw6tdq2Wm0XoLaVqpxtO8vMpg5VqGrYP/HgZJuZtebWAEettq1W2wWobaWqVtv0NF4kEgq7SCTyDvvqnB/fU6ttq9V2AWpbqarStlxfs4tI9eR9ZheRKlHYRSKRS9hJXkPyDZK7Sd6VRxtCSO4luY3kFpJtObdlDclOktsHbWshuYHkW8nnIdfYy6lt95DsSI7dFpIrcmrbHJLPk9xJcgfJ25PtuR47p11VOW5Vf81OsgDgTQBfAdAO4BUAK81sZ1UbEkByL4BWM8v9AgySlwE4AeDHZnZesu37AI6Y2b3JP8pJZnZnjbTtHgAn8l7GO1mtaMbgZcYB3ADg68jx2Dnt+iqqcNzyOLMvBbDbzPaYWQ+AnwK4Pod21DwzexHAkY9tvh7A2uTrtRj4Y6m6QNtqgpkdMLNXk6+PA/homfFcj53TrqrII+yzAOwf9H07amu9dwPwLMnNJFfl3ZghTDezA8nXBwFMz7MxQ0hdxruaPrbMeM0cu1KWP89Kb9B90qVm9gUA1wL4ZvJ0tSbZwGuwWuo7HdYy3tUyxDLjv5LnsSt1+fOs8gh7B4A5g76fnWyrCWbWkXzuBLAOtbcU9aGPVtBNPnfm3J5fqaVlvIdaZhw1cOzyXP48j7C/AmABybNJNgK4GcD6HNrxCSSbkzdOQLIZwFWovaWo1wO4Jfn6FgBP59iW31Ary3iHlhlHzscu9+XPzazqHwBWYOAd+bcB/GUebQi0ax6A15KPHXm3DcBjGHha14uB9zZuBTAZwEYAbwF4DkBLDbXtEQDbAGzFQLBm5NS2SzHwFH0rgC3Jx4q8j53TrqocN10uKxIJvUEnEgmFXSQSCrtIJBR2kUgo7CKRUNhFIqGwi0Ti/wEyIWtF5oxGCQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = data_train_list[0]\n",
    "plt.imshow(np.reshape(sample[0], (28, 28)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fcb32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.double) -> np.double:\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x: np.double) -> np.double:\n",
    "    \"\"\"Производная сигмоидной функции.\"\"\"\n",
    "    a = sigmoid(x)\n",
    "    return a * (1 - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c33f90a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticCost:\n",
    "    def fn(a, y):\n",
    "        return 0.5 * np.linalg.norm(a - y)**2\n",
    "    def delta(x, a, y):\n",
    "        return (a - y) * sigmoid_prime(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cc3111b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (26,8) (26,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (grad_b, grad_w)\n\u001b[1;32m     71\u001b[0m net \u001b[38;5;241m=\u001b[39m Network()\n\u001b[0;32m---> 72\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mNetwork.SGD\u001b[0;34m(self, data_train_list, data_test_list, epochs, n_batch, lr, reg)\u001b[0m\n\u001b[1;32m     23\u001b[0m batch_list \u001b[38;5;241m=\u001b[39m [ data_train_list[k:k\u001b[38;5;241m+\u001b[39mn_batch] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m r ]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batch_list:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# learning rate\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mreg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mn_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mn_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mNetwork.update_batch\u001b[0;34m(self, batch, lr, reg, n_train, n_batch)\u001b[0m\n\u001b[1;32m     37\u001b[0m     delta_grad_b, delta_grad_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_backward_propagation(x, y)\n\u001b[1;32m     38\u001b[0m     grad_b \u001b[38;5;241m=\u001b[39m [b\u001b[38;5;241m+\u001b[39mdgb \u001b[38;5;28;01mfor\u001b[39;00m b, dgb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grad_b, delta_grad_b)]\n\u001b[0;32m---> 39\u001b[0m     grad_w \u001b[38;5;241m=\u001b[39m [w\u001b[38;5;241m+\u001b[39mdgw \u001b[38;5;28;01mfor\u001b[39;00m w, dgw \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grad_w, delta_grad_w)]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m (reg \u001b[38;5;241m/\u001b[39m n)) \u001b[38;5;241m*\u001b[39m w \u001b[38;5;241m-\u001b[39m (eta \u001b[38;5;241m/\u001b[39m n_batch) \u001b[38;5;241m*\u001b[39m gw \u001b[38;5;28;01mfor\u001b[39;00m w, gw \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, grad_w)]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases \u001b[38;5;241m=\u001b[39m [b \u001b[38;5;241m-\u001b[39m (lr \u001b[38;5;241m/\u001b[39m n_batch) \u001b[38;5;241m*\u001b[39m gb \u001b[38;5;28;01mfor\u001b[39;00m b, gb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases, grad_b)]\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m     delta_grad_b, delta_grad_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_backward_propagation(x, y)\n\u001b[1;32m     38\u001b[0m     grad_b \u001b[38;5;241m=\u001b[39m [b\u001b[38;5;241m+\u001b[39mdgb \u001b[38;5;28;01mfor\u001b[39;00m b, dgb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grad_b, delta_grad_b)]\n\u001b[0;32m---> 39\u001b[0m     grad_w \u001b[38;5;241m=\u001b[39m [\u001b[43mw\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mdgw\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w, dgw \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grad_w, delta_grad_w)]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m (reg \u001b[38;5;241m/\u001b[39m n)) \u001b[38;5;241m*\u001b[39m w \u001b[38;5;241m-\u001b[39m (eta \u001b[38;5;241m/\u001b[39m n_batch) \u001b[38;5;241m*\u001b[39m gw \u001b[38;5;28;01mfor\u001b[39;00m w, gw \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, grad_w)]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases \u001b[38;5;241m=\u001b[39m [b \u001b[38;5;241m-\u001b[39m (lr \u001b[38;5;241m/\u001b[39m n_batch) \u001b[38;5;241m*\u001b[39m gb \u001b[38;5;28;01mfor\u001b[39;00m b, gb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases, grad_b)]\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (26,8) (26,) "
     ]
    }
   ],
   "source": [
    "class Network:\n",
    "    def __init__(self, layer_sizes=[784,8,8,26]):\n",
    "        self.layer_count = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.cost_function = QuadraticCost\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.layer_sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) / np.sqrt(x)\n",
    "                        for x, y in zip(self.layer_sizes[:-1], self.layer_sizes[1:])]\n",
    "    \n",
    "    def SGD(self,\n",
    "            data_train_list=data_train_list,\n",
    "            data_test_list=data_test_list,\n",
    "            epochs=1,\n",
    "            n_batch=10,\n",
    "            lr=3.0,\n",
    "            reg = 0.0):\n",
    "        n_train = len(data_train_list)\n",
    "        n_test = len(data_test_list)\n",
    "        for e in range(epochs):\n",
    "            \"\"\"\n",
    "            В одной epoch можно использовать только 80% всех данных.\n",
    "            \"\"\"\n",
    "            np.random.shuffle(data_train_list)\n",
    "            r = range(0, int(n_train * 0.8), n_batch)\n",
    "            batch_list = [ data_train_list[k:k+n_batch] for k in r ]\n",
    "            for batch in batch_list:\n",
    "                self.update_batch(batch=batch,\n",
    "                                  lr=lr, # learning rate\n",
    "                                  reg=reg,\n",
    "                                  n_train=n_train,\n",
    "                                  n_batch=n_batch)\n",
    "    def update_batch(self, batch, lr, reg, n_train, n_batch):\n",
    "        \"\"\"\n",
    "        Будем возвращать градиент функции затрат в виде 'grad_b' и 'grad_w'.\n",
    "        \"\"\"\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in batch:\n",
    "            delta_grad_b, delta_grad_w = self.forward_backward_propagation(x, y)\n",
    "            grad_b = [b+dgb for b, dgb in zip(grad_b, delta_grad_b)]\n",
    "            grad_w = [w+dgw for w, dgw in zip(grad_w, delta_grad_w)]\n",
    "        self.weights = [(1 - lr * (reg / n)) * w - (eta / n_batch) * gw for w, gw in zip(self.weights, grad_w)]\n",
    "        self.biases = [b - (lr / n_batch) * gb for b, gb in zip(self.biases, grad_b)]\n",
    "\n",
    "    def forward_backward_propagation(self, x, y):\n",
    "        \"\"\"\n",
    "        Шагаем вперед.\n",
    "        \"\"\"\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        z_array = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            z_array.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \"\"\"\n",
    "        Шагаем назад.\n",
    "        \"\"\"\n",
    "        delta = self.cost_function.delta(z_array[-1], activations[-1], y)\n",
    "        grad_b[-1] = delta\n",
    "        grad_w[-1] = np.dot(delta, activation[-1].T)\n",
    "        \n",
    "        for l in range(2, self.layer_count):\n",
    "            sp = sigmoid_prime(z_array[-l])\n",
    "            delta = np.dot(self.weights[-l + 1].T, delta) * sp\n",
    "            grad_b[-l] = delta\n",
    "            grad_w[-l] = np.dot(delta, activations[-l - 1].T)\n",
    "        return (grad_b, grad_w)\n",
    "\n",
    "net = Network()\n",
    "net.SGD()\n",
    "# print(len(batch_list), len(batch_list[0]), batch_list[0][0][1], n_train, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa3722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weight dimensions:\")\n",
    "for i, w in enumerate(net.weights):\n",
    "    print( f\"W{i}: {len(w[0])} x {len(w)}\" )\n",
    "\"\"\"\n",
    "Смещение (bias) отсутствует в сенсорном слое.\n",
    "\"\"\"\n",
    "print(\"Bias dimensions:\")\n",
    "for i, b in enumerate(net.biases):\n",
    "    print( f\"L{i + 1}: {len(b[0])} x {len(b)}\" )\n",
    "Image(filename='./media/img0.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
